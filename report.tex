% A simple template for LaTeX documents
% 
% To produce pdf run:
%   $ pdflatex paper.tex 
%

\documentclass[10pt, twocolumn]{article}
%\documentclass[12pt]{article}

% Begin paragraphs with new line
\usepackage{parskip}  

% Change margin size
\usepackage[margin=1in]{geometry}   

% Graphics Example:  (PDF's make for good plots)
\usepackage{graphicx}               
% \centerline{\includegraphics{figure.pdf}}

% Allows hyperlinks
\usepackage{hyperref}

% Blocks of code
\usepackage{listings}
\lstset{basicstyle=\ttfamily, title=\lstname}
% Insert code like this. replace `plot.R` with file name.
% \lstinputlisting{plot.R}

% Monospaced fonts
%\usepackage{inconsolata}
% GNU \texttt{make} is a nice tool.

% Supports proof environment
\usepackage{amsthm}

% Allows writing \implies and align*
\usepackage{amsmath}

% Allows mathbb{R}
\usepackage{amsfonts}

% Numbers in scientific notation
\usepackage{siunitx}

% Use tables generated by pandas
\usepackage{booktabs}

% norm and infinity norm
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\inorm}[1]{\left\lVert#1\right\rVert_\infty}

% Statistics essentials
\newcommand{\Expect}{\operatorname{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\section{Distributions}

\subsection{Binomial}

Sum of $n$ bernoulli trials with probability of success $p$.

$X \sim B(n, p)$
\[
    p(k) = \binom{n}{k} p^k (1-p)^{n-k}
    \qquad k = 0, 1, \dots, n
\]

mgf: $M_X (t) = (pe^t + 1 - p)^n$

$\Expect X = np, \quad \Var X = np(1-p)$


\subsection{Poisson}

$X \sim P(\lambda)$
\[
    p(k) = \frac{e^{-\lambda} \lambda^k}{k!}
    \qquad k = 0, 1, \dots
\]

mgf: $M_X (t) = e^{\lambda (e^t -1)}$

$\Expect X = \lambda, \quad \Var X = \lambda$


\subsection{Multivariate Normal}

$X \sim N(\mu, \Sigma)$, $\Sigma$ positive definite
\[
    f(x) = \frac{\exp\{ - \frac{1}{2}(x - \mu)^T \Sigma^{-1} (x - \mu) \}}
        {(2\pi)^{\frac{k}{2}} \sqrt{\det(\Sigma)}}
\]

\subsection{Gamma}

$X \sim \text{Gamma}(p, \lambda)$
\[
    f(x) = \frac{\lambda^p x^{p-1} e^{-\lambda x}}{\Gamma(p)}
    \qquad x > 0
\]
mgf: 

$X \sim \text{Gamma}(p, \lambda) \iff \lambda X \sim \text{Gamma}(p, 1)$

Gamma function: $\Gamma(p) = \int_0^\infty t^{p-1} e^{-t} dt$.

$\Gamma(\frac{1}{2}) = \sqrt{\pi}$.

$\Gamma(p + 1) = p \Gamma(p)$

$\Gamma(k) = (k-1)!$ for $k$ positive integer.

\subsection{Chi square}

$\chi^2_n$
Special case: $\chi^2_n \equiv \text{Gamma}(\frac{n}{2}, \frac{1}{2})$

Let $Z_i$ be iid $N(0, 1)$.

$\sum_{i=1}^n Z_i^2 \sim \chi^2_n$

Noncentral $\chi^2$. Let $Y \sim N(\mu, I)$ be an $n$ vector. Then 
\[
    \norm{Y}^2 \sim \chi^2_n(\norm{\mu}^2)
\]

F

T

\subsection{Studentized range distribution}

Let $z_1, \dots z_K$ be i.i.d $N(0, 1)$ and
$X$ be an independent $\chi^2_\nu$. Then
\[
    \frac{\max_i z_i - \min_i z_i}{\sqrt(X / \nu)} \sim q_{K, \nu}
\]



\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Math}

\subsection{Multivariate Normal}

$X \sim N(\mu, \Sigma)$, $A$ an $m \times n$ matrix,
then 
\[
    AX \sim N(A \mu, A \Sigma A^t)
\]
For $\Sigma$ full rank it's possible to transform between $z \sim
N(0, I)$ and $X$:
\[
    X = \Sigma^{1/2} z + \mu \qquad z = \Sigma^{-1/2} (X - \mu)
\]

Conditional distribution:


\subsection{Conditional Distributions}

Conditional pdf:
\[
    f_{x|y}(x, y) \equiv \frac{f_{x, y}(x, y)}{f_y(y)}
\]

Iterated expectation:
\[
    E(Y) = E(E(Y | X))
\]

Conditional variance formula:
\[
    \Var(Y) = \Var(E(Y | X)) + E(\Var(Y | X))
\]

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Applied}

Matrix / Vector differentiation

\subsection{Linear Models}

Least Squares Principle
\[
    \text{arg min}_\beta \norm{Y - X\beta}^2
\]

Normal Model 
\[
    Y = X\beta + \epsilon, \qquad \epsilon \sim N(0, \sigma^2 I)
\]

Normal Equations - Any $b$ satistfying this solves the least squares
\[
    X^T X b = X^T y
\]

Gauss Markov Theorem - $\hat{\beta} (X^T X)^{-1} y$ is Best Linear Unbiased
Estimator (BLUE) of $\beta$.

Estimating the variance: $\frac{\norm{y - X \hat{\beta}}^2}{\sigma^2} \sim
\chi^2_{n-p}$.
\[
    \hat{\sigma}^2 = \frac{\norm{y - X \hat{\beta}}^2}{n - p}
\]

Use t test for hypothesis testing and confidence intervals for the value of
a particular $\beta_j$ coefficient.
\[
    \frac{\beta_j - \beta_j^*}{\hat{\sigma} \sqrt{w_{ii}}} \sim t_{n-p}
\]

General linear tests. Partition $\beta = (\beta_1, \beta_2)$ where $\beta_1$
is an $r$ vector and $\beta_2$ is $p - r$. Null hypothesis $H_0: \beta_2 =
\beta_2^*$ (often 0), and $H_a: \beta_2 \neq \beta_2^*$. Then
$SSE_r = \norm{y - X_2 \beta_2^* - X_1 \tilde{\beta_1}}^2$ is the sum of
squared error for the reduced model and 
$SSE_f = \norm{y - X \hat{\beta}}^2$ is the squared sum of error for the
full model.
Under $H_0$:
\[
    \frac{\frac{SSE_r - SSE_f}{p - r}}
         {\frac{SSE_f}{n - p}}
         \sim F_{p-r, n-p}
\]

Alternate forms of linear test, and testing a linear combination if $R\beta
= r$.
\[
    \frac{(R \hat{\beta} - r)^T (R(X^T X)^{-1} R^T)^{-1} (R \beta - r) / s}
    {\hat{\sigma}}^2 \sim F_{s, n-p}
\]

Confidence intervals for new observation $Y_h$ and $E[Y_h]$.

Simultaneous confidence bands for all $f(x)$.

If the model has an intercept then $SSTO = SSR + SSE$.
\[
    \sum (y_i - \bar{y})^2 = \sum (\hat{y_i} = \bar{y})^2 + (yi -
    \hat{yi})^2
\]

$R^2$ and adjusted $R^2$

\subsection{ANOVA}

Three principles of experimental design: 1) Replication 2) Randomization 3)
Blocking

One way ANOVA with $n$ total observations, $K$ groups:

{
\centering
\begin{tabular}{lll}
    SS   &  & DF     \\
    SSTR & $\sum_{j=1}^K n_j (\bar{y_{j \cdot}} - \bar{y_{\cdot \cdot}})^2$  & K - 1 \\
    SSE  & $\sum_{i=1}^n (y_{ij} - \bar{y_{j \cdot}})^2$  & n - K \\
    SSTO & $\sum_{i=1}^n (y_{ij} - \bar{y_{\cdot \cdot}})^2$  & n - 1 
\end{tabular}
}

Contrasts are sums of the form $\Phi = \sum_{i=1}^K c_i \mu_i$ with
$\sum_{i=1}^K c_i = 0$.
Tukey's works for all pairwise contrasts.
Scheffe's and extended Tukey works for all contrasts.
Bonferroni's is for a limited number of pre specified contrasts.

\subsection{General Techniques}

Singular Value Decompostion (SVD) Any matrix $X$ can be written
\[
    X = UDV^T
\]
with $U, V$ orthogonal, and $D$ diagonal.

Moore Penrose Psuedoinverse $A^+$ exists uniquely for every matrix $A$.

Projection matrix $P$ are symmetric and idempotent. They have eigenvalues
either 0 or 1.
\[
    P = P^T \qquad P^2 = P
\]

Covariance of linear transformations
\[
    Cov(Ay, Bx) = A Cov(y, x) B^T
\]


\newpage

\section{Lecture topics}

\subsection{Applied}

4 Nov - Interaction plots for two way ANOVA with balanced design, Linear
models with random $X$.

2 Nov - Midterm

\subsection{Math}

2 Nov - Location-scale families, invariance, ancillary and sufficient
statistics, order statistics, multinomial distribution.

4 Nov - Midterm

\end{document}
