% A simple template for LaTeX documents
% 
% To produce pdf run:
%   $ pdflatex paper.tex 
%

\documentclass[12pt]{article}

% Begin paragraphs with new line
\usepackage{parskip}  

% Change margin size
\usepackage[margin=1in]{geometry}   

% Graphics Example:  (PDF's make for good plots)
\usepackage{graphicx}               
% \centerline{\includegraphics{figure.pdf}}

% Allows hyperlinks
\usepackage{hyperref}

% Blocks of code
\usepackage{listings}
\lstset{basicstyle=\ttfamily, title=\lstname}
% Insert code like this. replace `plot.R` with file name.
% \lstinputlisting{plot.R}

% Monospaced fonts
%\usepackage{inconsolata}
% GNU \texttt{make} is a nice tool.

% Supports proof environment
\usepackage{amsthm}

% Allows writing \implies and align*
\usepackage{amsmath}

% Allows mathbb{R}
\usepackage{amsfonts}

% Numbers in scientific notation
\usepackage{siunitx}

% Use tables generated by pandas
\usepackage{booktabs}

% norm and infinity norm
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\inorm}[1]{\left\lVert#1\right\rVert_\infty}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\section{Math}

\newpage

\section{Applied}

Topics covered up to 31 October

Matrix / Vector differentiation

\subsection{Linear Models}

Least Squares Principle
\[
    \text{arg min}_\beta \norm{Y - X\beta}^2
\]

Normal Model 
\[
    Y = X\beta + \epsilon, \qquad \epsilon \sim N(0, \sigma^2 I)
\]

Normal Equations - Any $b$ satistfying this solves the least squares
\[
    X^T X b = X^T y
\]

Gauss Markov Theorem - $\hat{\beta} (X^T X)^{-1} y$ is Best Linear Unbiased
Estimator (BLUE) of $\beta$.

Estimating the variance: $\frac{\norm{y - X \hat{\beta}}^2}{\sigma^2} \sim
\chi^2_{n-p}$.
\[
    \hat{\sigma}^2 = \frac{\norm{y - X \hat{\beta}}^2}{n - p}
\]

Use t test for hypothesis testing and confidence intervals for the value of
a particular $\beta_j$ coefficient.
\[
    \frac{\beta_j - \beta_j^*}{\hat{\sigma} \sqrt{w_{ii}}} \sim t_{n-p}
\]

General linear tests. Partition $\beta = (\beta_1, \beta_2)$ where $\beta_1$
is an $r$ vector and $\beta_2$ is $p - r$. Null hypothesis $H_0: \beta_2 =
\beta_2^*$ (often 0), and $H_a: \beta_2 \neq \beta_2^*$. Then
$SSE_r = \norm{y - X_2 \beta_2^* - X_1 \tilde{\beta_1}}^2$ is the sum of
squared error for the reduced model and 
$SSE_f = \norm{y - X \hat{\beta}}^2$ is the squared sum of error for the
full model.
Under $H_0$:
\[
    \frac{\frac{SSE_r - SSE_f}{p - r}}
         {\frac{SSE_f}{n - p}}
         \sim F_{p-r, n-p}
\]

Alternate forms of linear test, and testing a linear combination if $R\beta
= r$.
\[
    \frac{(R \hat{\beta} - r)^T (R(X^T X)^{-1} R^T)^{-1} (R \beta - r) / s}
    {\hat{\sigma}}^2 \sim F_{s, n-p}
\]

Confidence intervals for new observation $Y_h$ and $E[Y_h]$.

Simultaneous confidence bands for all $f(x)$.

If the model has an intercept then $SSTO = SSR + SSE$.
\[
    \sum (y_i - \bar{y})^2 = \sum (\hat{y_i} = \bar{y})^2 + (yi -
    \hat{yi})^2
\]

$R^2$ and adjusted $R^2$

\subsection{ANOVA}

Three principles of experimental design: 1) Replication 2) Randomization 3)
Blocking

One way ANOVA with $n$ total observations, $K$ groups:

{
\centering
\begin{tabular}{lll}
    SS   &  & DF     \\
    SSTR & $\sum_{j=1}^K n_j (\bar{y_{j \cdot}} - \bar{y_{\cdot \cdot}})^2$  & K - 1 \\
    SSE  & $\sum_{i=1}^n (y_{ij} - \bar{y_{j \cdot}})^2$  & n - K \\
    SSTO & $\sum_{i=1}^n (y_{ij} - \bar{y_{\cdot \cdot}})^2$  & n - 1 
\end{tabular}
}

Contrasts are sums of the form $\Phi = \sum_{i=1}^K c_i \mu_i$ with
$\sum_{i=1}^K c_i = 0$.
Tukey's works for all pairwise contrasts.
Scheffe's and extended Tukey works for all contrasts.
Bonferroni's is for a limited number of pre specified contrasts.

\subsection{General Techniques}

Singular Value Decompostion (SVD) Any matrix $X$ can be written
\[
    X = UDV^T
\]
with $U, V$ orthogonal, and $D$ diagonal.

Moore Penrose Psuedoinverse $A^+$ exists uniquely for every matrix $A$.

Projection matrix $P$ are symmetric and idempotent. They have eigenvalues
either 0 or 1.
\[
    P = P^T \qquad P^2 = P
\]

Covariance of linear transformations
\[
    Cov(Ay, Bx) = A Cov(y, x) B^T
\]

\subsection{Distributions}

Multivariate Normal $y \sim N(\mu, \Sigma)$, $A$ an $m \times n$ matrix,
then 
\[
    Ay \sim N(A \mu, A \Sigma A^t)
\]

Noncentral $\chi^2$. Let $Y \sim N(\mu, I)$ be an $n$ vector. Then 
\[
    \norm{Y}^2 \sim \chi^2_n(\norm(\mu)^2)
\]

F

T

Studentized range distribution. Let $z_1, \dots z_K$ be i.i.d $N(0, 1)$ and
$X$ be an independent $\chi^2_\nu$. Then
\[
    \frac{\max_i z_i - \min_i z_i}{\sqrt(X / \nu)} \sim q_{K, \nu}
\]
\end{document}
