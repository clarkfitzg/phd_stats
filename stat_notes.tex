% A simple template for LaTeX documents
% 
% To produce pdf run:
%   $ pdflatex paper.tex 
%

\documentclass[10pt, twocolumn]{article}
%\documentclass[12pt]{article}

% Begin paragraphs with new line
\usepackage{parskip}  

% Change margin size
\usepackage[margin=0.5in]{geometry}   

% Graphics Example:  (PDF's make for good plots)
\usepackage{graphicx}               
% \centerline{\includegraphics{figure.pdf}}

% Allows hyperlinks
\usepackage{hyperref}

% Blocks of code
\usepackage{listings}
\lstset{basicstyle=\ttfamily, title=\lstname}
% Insert code like this. replace `plot.R` with file name.
% \lstinputlisting{plot.R}

% Monospaced fonts
%\usepackage{inconsolata}
% GNU \texttt{make} is a nice tool.

% Supports proof environment
\usepackage{amsthm}

% Allows writing \implies and align*
\usepackage{amsmath}

% Allows mathbb{R}
\usepackage{amsfonts}

% Numbers in scientific notation
\usepackage{siunitx}

% Use tables generated by pandas
\usepackage{booktabs}

% norm and infinity norm
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\inorm}[1]{\left\lVert#1\right\rVert_\infty}

% Statistics essentials
\newcommand{\iid}{\text{ iid }}
\newcommand{\Expect}{\operatorname{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\textbf{Binomial}
$X \sim B(n, p)$
\[
    p(k) = \binom{n}{k} p^k (1-p)^{n-k}
    \qquad k = 0, 1, \dots, n
\]
$\Expect X = np, \quad \Var X = np(1-p)$

mgf: $M_X (t) = (pe^t + 1 - p)^n$

Beta is conjugate prior, Fisher info $I(p) = \frac{1}{p(1 - p)}$

\textbf{Poisson}
$X \sim P(\lambda)$
\[
    p(k) = \frac{e^{-\lambda} \lambda^k}{k!}
    \qquad k = 0, 1, \dots
\]
$\Expect X = \lambda, \quad \Var X = \lambda$

mgf: $M_X (t) = e^{\lambda (e^t -1)}$ Use recursive relation to compute
$\Expect(X_i)$.

Gamma is conjugate prior, Fisher info $I(\lambda) = \frac{1}{\lambda}$

\textbf{Normal}
$X \sim N(\mu, \Sigma)$, $\Sigma$ positive definite
\[
    f(x) = \frac{\exp\{ - \frac{1}{2}(x - \mu)^T \Sigma^{-1} (x - \mu) \}}
        {(2\pi)^{\frac{k}{2}} \sqrt{\det(\Sigma)}}
        = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x - \mu)^2}{2
        \sigma^2}}
\]
mgf: $M_X (t) = \exp (\mu' t + \frac{1}{2} t' \Sigma t)$

Normal is conj. prior, Fisher info $I(\mu, \sigma^2) = 
[\begin{smallmatrix}
        1 / \sigma^2 & 0 \\
        0 & 1 / 2\sigma^4 \\
\end{smallmatrix}]$

\textbf{Beta}
$ X \sim \text{Beta}(\alpha, \beta)$
\[
    f(x) = \frac{x^{\alpha-1}(1 - x)^{\beta-1}}{B(\alpha, \beta)} 
    \qquad 0 \leq x \leq 1
\]
$\Expect X = \frac{\alpha}{\alpha + \beta},
\quad \Var X = \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}$

using the beta function:
\[
    B(\alpha, \beta) =
    \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha+ \beta)} =
    \int_0^1 t^{\alpha -1} (1-t)^{\beta - 1}dt
\]

\textbf{Gamma}
$X \sim \text{Gamma}(\alpha, \beta)$
\[
    f(x) = \frac{\beta^\alpha x^{\alpha-1} e^{-\beta x}}{\Gamma(\alpha)}
    \qquad x > 0
\]
$\Expect X = \frac{\alpha}{\beta},
\quad \Var X = \frac{\alpha}{\beta^2}$

mgf: $M_X (t) = (1 - \frac{t}{\beta})^{-\alpha}, t < \beta$

$X \sim \text{Gamma}(\alpha, \beta) \iff \beta X \sim \text{Gamma}(\alpha, 1)$

$X_i \iid \text{Gamma}(\alpha_i, \beta)$, then
\[
    \sum X_i \sim \text{Gamma}(\sum \alpha_i, \beta)
\]
Gamma function: $\Gamma(\alpha) = \int_0^\infty t^{\alpha-1} e^{-t} dt$.

$\Gamma(\frac{1}{2}) = \sqrt{\pi}$.

$\Gamma(\alpha + 1) = \alpha \Gamma(\alpha)$

$\Gamma(k) = (k-1)!$ for $k$ positive integer.

\textbf{Exponential}
Special case: $X \sim \text{Exp}(\lambda) \equiv \text{Gamma}(1, \lambda)$

$
f(x) = \lambda e^{-\lambda x},
\quad x > 0
\qquad \Expect X = \frac{1}{\lambda},
\quad \Var X = \frac{1}{\lambda ^2}
$

CDF $F(x) = 1 - e^{-\lambda x}$

\textbf{Chi square}
Special case: $X \sim \chi^2_n \equiv \text{Gamma}(\frac{n}{2}, \frac{1}{2})$

$
f(x) \propto x^{\frac{n}{2} - 1} e^{\frac{-x}{2}},
\quad x > 0
\qquad \Expect X = n,
\quad \Var X = 2n
$

Let $Z_i$ be iid $N(0, 1)$.

$\sum_{i=1}^n Z_i^2 \sim \chi^2_n$

Noncentral $\chi^2$. Let $Y \sim N(\mu, I)$ be an $n$ vector. Then 
\[
    \norm{Y}^2 \sim \chi^2_n(\norm{\mu}^2)
\]

\textbf{F}
\[
    F(m, n) \equiv \frac{\frac{\chi^2_m}{m}}
        {\frac{\chi^2_n}{n}}
\]
Where numerator and denominator are independent $\chi^2$.

\textbf{T}
\[
    t(n) = \frac{N(0, 1)}
    {\sqrt{\frac{\chi^2_n}{n}}}
\]
Where numerator and denominator are independent.

\vspace{0.2in}
\hrule

\textbf{Transformations} If $g$ 1:1 with continuous derivatives and nonzero
Jacobian, and $Y = g(X)$, then the density
\[
    f_Y(y) = f_X(g^{-1}(y)) |J_{g^{-1}}(y)|
\]
For affine transformation $Y = AX + c$ then 
\[
    f_Y(y) = f_X(A^{-1}(y - c)) |\det A|^{-1}
\]

Moment generating functions determine distribution
\[
    M_X(t) \equiv \Expect (e^{tX}),
    \quad M_X'(0) = \Expect(X)
\]
$X_i$ independently distributed $\iff$
\[
    M_{\sum X_i} (t) = \prod M_{X_i} (t)
\]
\textbf{Characteristic function}
\[
    \phi(t) = \Expect (e^{i t^T X)}
    = \Expect (\cos (t^T X)) + i \Expect(\sin(t^T X))
\]
Order statistics for sorted sample $X_{(1)}, \dots, X_{(n)}$ has pdf:
\[
    n! \prod_{i=1}^n f(X_{(i)}) \quad I(X_{(1)} < \dots < X_{(n)})
\]

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{TODO - add measure theory}

\textbf{Jensen's Inequality} if $S \subset R^k$ convex and closed, $g$ convex on $S$, $P[X
\in S] = 1$, and $\Expect X$ is finite, then $\Expect X \in S$, $\Expect
g(X)$ exists, and
\[
    \Expect g(X) \geq g(\Expect X)
\]
\textbf{Holder's Inequality} if $r, s > 1$ and $\frac{1}{r} + \frac{1}{s} =
1$ then
\[
    \Expect |XY| \leq (\Expect |X|^r)^{\frac{1}{r}}(\Expect |X|^s)^{\frac{1}{s}}
\]

$T(X)$ Sufficient means the distribution of $X | T(X)$ does not depend on
$\theta$.

Factorization theorem: $T(x)$ is sufficient $\iff$
\[
    f_{\theta}(x) = h(x) g(\theta, T(x))
\]
$L_x (\theta) = p_\theta (x) = p(x, \theta)$ likelihood is function of
$\theta$, density is function of $x$.

The likelihood ratio
\[
    \lambda_x (\theta) = \frac{L_x (\theta)}{L_x (\theta_0)}
\]
is minimal sufficient. To show $T(x)$ is minimal sufficient show that it is
sufficient and a function of the likelihood $\lambda_x (\theta)$.

\textbf{Fisher information}
\[
    I(\theta) = \Expect_\theta \left[ \frac{\partial}{\partial \theta} 
    \log L_X (\theta) \right]^2
    = \Expect_\theta \left[ - \frac{\partial^2}{\partial \theta^2}
    \log L_X (\theta) \right]
\]
$\text{bias } \hat{v} \equiv \Expect (\hat{v}) - v$
\[
    MSE(\hat{v}) \equiv \Expect (\hat{v} - v)^2 
    = \Var (\hat{v}) + (\text{bias } \hat{v})^2
\]
\textbf{Rao-Blackwell} Let $S(X)$ be an unbiased point estimator for
$g(\theta)$. Conditioning on a sufficient statistic $T(X)$ reduces
variance.
\[
    \Var_{\theta} (S(X)) \geq \Var_{\theta} (\Expect (S(X) | T(X)))
\]
Also holds for more general convex loss function $L$:
\[
    R(\theta, S) \equiv \Expect_{\theta} L(\theta, S(X)) \geq
    \Expect_{\theta} L(\theta, \Expect( S(X) | T(X)))
\]
\textbf{Completeness} $T(X)$ is complete if $\Expect g(T(X)) = 0$ 
implies $g = 0$ almost surely for all $\theta$.

\textbf{Cramer Rao Inequality} Let $g: \Theta \rightarrow R$. Suppose there
exists an unbiased estimator $U(X)$, $\Expect U(X) = g(\theta)$. Then
\[
    \Var_\theta U(X) \geq 
    \left( \frac{\partial g(\theta)}{\partial \theta} \right)^T
    I(\theta)^{-1}
    \left( \frac{\partial g(\theta)}{\partial \theta} \right)
\]
Basu's Theorem - If $T(X)$ complete sufficient statistic and $A(X)$ is 
ancillary then $A(X)$ and $T(X)$ are independent.

\textbf{Lehmann - Scheffe} Suppose $T(X)$ is complete sufficient. Then
there exists unique unbiased estimator $\Expect h(T(x))$ of $g(\theta) \in
R$ with smallest variance (MVUE). 

\textbf{Exponential Families} $T(x)$ is natural sufficient statistic and is
complete sufficient if the $k$ parameter exponential family is full rank.
\[
    p(x, \theta) = h(x) \exp \{ \eta(\theta)^T T(x) - B(\theta) \}
\]
Canonical form model indexed by $\eta$.
\[
    q(x, \eta) = h(x) \exp \{ \eta^T T(x) - A(\eta) \}
\]
\[
    \dot{A} (\eta) = \Expect_\eta (T(X)) \quad
    \ddot{A} (\eta) = I(\eta) = \Var_\eta (T(X))
\]
Then moment generating function for $T(X)$ is
\[
    M_{T(X)}(t) = \exp \{A(t + \eta) - A(\eta) \}
\]
Equivalent statements useful for GLM's such as $Y \sim N(X \beta,
\sigma_0^2 I)$, where $Z$ is $n \times p$:

1. $I(\beta) = \frac{1}{\sigma_0^2} X^T X$ positive definite
2. rank$(X) = p$
3. model is identifiable. More generally another equivalent statement is
$\Var (T(X)) = \ddot{A} (\eta)$ is positive definite.

\subsection*{Decision Theory}

\textbf{Decision rule} $\delta: \mathcal{X} \rightarrow \mathcal{A}$, where 
$\delta \in \mathcal{D}$, the space of possible decision rules and
$\mathcal{A}$ is the action space.

\textbf{Loss function} $l: \Theta \times \mathcal{A} \rightarrow \mathbb{R}^+$
Posterior mean minimizes square error loss; median minimizes absolute loss.

\textbf{Risk function} $R: \Theta \times \mathcal{D} \rightarrow
\mathbb{R}^+$ expected loss for a particular value of $\theta$
\[
    R(\theta, \delta) = \Expect_\theta l(\theta, \delta(X)) = \int
    l(\theta, \delta(x)) \cdot p_\theta (x) dx
\]
Bayes setup:
\[
    \pi(\theta | x) = \frac{p_\theta (x) \pi(\theta)}{m(x)}
\]
\textbf{Bayes decision rule} If there exists $\delta_\pi \in \mathcal{D}$
w.r.t prior $\pi$ such that
\[
    r(\pi, \delta_\pi) = \inf_{\delta \in \mathcal{D}} r(\pi, \delta)
\]
To find Bayes rule minimize the posterior risk:
\[
    \delta_\pi (x) = \min_{a \in \mathcal{A}} r_\pi (a | x)
\]

\textbf{Bayes risk} $r_\pi : \mathcal{D} \rightarrow
\mathbb{R}^+$ expected loss for fixed prior $\pi$
\[
    r_\pi (\delta) = \Expect_\pi R(\theta, \delta) 
    = \int_\Theta R(\theta, \delta) \pi (d \theta)
    = \int_\mathcal{X} r_\pi(\delta(x) | x) m(x) dx
\]
To find Bayes risk: 1) find the Bayes rule 2) compute the risk function 3)
take the expectation of the risk wrt prior $\pi$.


\textbf{Minimax} decision rule $\delta^*$ minimizes the worst case
scenario, satisfies
\[
    \sup_{\theta} R(\theta, \delta^*) = \inf_{\delta} \sup_{\theta}
    R(\theta, \delta)
\]
To show $\delta^*$ is minimax, first check for constant risk $R(\theta,
\delta^* = c$ for all $\theta$, then find a
prior $\pi$ such that $\delta^*$ is the Bayes rule. This $\pi$ is least
favorable. More generally can find
a sequence of priors $(\pi_k)$ such that the Bayes risk $r_{\pi_k}
(\delta_{\pi_k}) \rightarrow c$.

\newpage

\subsection*{Multivariate Normal}

Stein's formula: $X \sim N(\mu, \sigma)$
\[
    \Expect (g(X) (X - \mu)) = \sigma^2 \Expect(g'(X))
\]
assuming these expectations are finite.

$X \sim N(\mu, \Sigma)$, $A$ an $m \times n$ matrix,
then 
\[
    AX \sim N(A \mu, A \Sigma A^t)
\]
For $\Sigma$ full rank it's possible to transform between $Z \sim
N(0, I)$ and $X$:
\[
    X = \Sigma^{1/2} Z + \mu \qquad Z = \Sigma^{-1/2} (X - \mu)
\]
In block matrix form:
\[
    X =
    \begin{bmatrix}
        X_1 \\
        X_2 \\
    \end{bmatrix}
    \sim N \left(
    \begin{bmatrix}
        \mu_1 \\
        \mu_2 \\
    \end{bmatrix}
    ,
    \begin{bmatrix}
        \Sigma_{11} & \Sigma_{12} \\
        \Sigma_{21} & \Sigma_{22} \\
    \end{bmatrix}
\right)
\]
Assuming $\Sigma_{11}$ is positive definite then the conditional
distribution
\[
    X_2 | X_1 \sim N(\mu_2 + \Sigma_{21} \Sigma_{11}^{-1} (X_1 - \mu_1),
    \Sigma_{22} - \Sigma_{21} \Sigma_{11}^{-1} \Sigma_{12})
\]

\subsection*{Conditional Distributions}

Conditional pdf:
\[
    f_{X|Y}(x, y) \equiv \frac{f_{X, Y}(x, y)}{f_Y(y)}
\]
Iterated expectation:
\[
    E(Y) = E(E(Y | X))
\]
Conditional variance formula:
\[
    \Var(Y) = \Var(E(Y | X)) + E(\Var(Y | X))
\]

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{General Techniques}

Singular Value Decompostion (SVD) Any matrix $X$ can be written
\[
    X = UDV^T
\]
with $U, V$ orthogonal, and $D$ diagonal.

Moore Penrose Psuedoinverse $A^+$ exists uniquely for every matrix $A$.

Projection matrix $P$ are symmetric and idempotent. They have eigenvalues
either 0 or 1.
\[
    P = P^T \qquad P^2 = P
\]

Covariance of linear transformations
\[
    Cov(Ay, Bx) = A Cov(y, x) B^T
\]
Invert $2 \times 2$ matrix:
$
    A = 
    [\begin{smallmatrix}
        a & b \\
        c & d \\
    \end{smallmatrix}]
$
\[
    A^{-1} = 
    \frac{1}{\det (A)}
    \begin{bmatrix}
        d & -b \\
        -c & a \\
    \end{bmatrix}
\]

Sum identities:
\[
    \sum_{k=0}^{\infty} p^k = \frac{p}{1 - p} \qquad 
    \sum_{k=0}^{\infty} k p^k = \frac{p}{(1 - p)^2} \qquad |p| < 1
\]

Integration by parts:
\[
    \int uv' = uv - \int u'v
\]

Matrix / Vector differentiation

$\frac{\partial A^T \beta}{\partial \beta} = A$, 
$\frac{\partial \beta^T A \beta}{\partial \beta} = (A + A^t) \beta =
2A\beta$ for $A$ symmetric.


\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Linear Models}

Least Squares Principle
\[
    \text{arg min}_\beta \norm{Y - X\beta}^2
\]

Normal Model 
\[
    Y = X\beta + \epsilon, \qquad \epsilon \sim N(0, \sigma^2 I)
\]

Normal Equations - Any $b$ satistfying this solves the least squares
\[
    X^T X b = X^T y
\]

Gauss Markov Theorem - $\hat{\beta}$ is Best Linear Unbiased
Estimator (BLUE) of $\beta$.
\[
    \hat{\beta} = (X^T X)^{-1} X^T y \sim N(\beta, \sigma^2 (X^T X)^{-1})
\]

Estimating the variance: $\frac{\norm{y - X \hat{\beta}}^2}{\sigma^2} \sim
\chi^2_{n-p}$.
\[
    \hat{\sigma}^2 = \frac{\norm{y - X \hat{\beta}}^2}{n - p}
\]

Use t test for hypothesis testing and confidence intervals for the value of
a particular $\beta_j$ coefficient. 
Let $w_{ii}$ be the $i$th diagonal entry of $(X^T X)^{-1}$.
\[
    \frac{\beta_j - \beta_j^*}{\hat{\sigma} \sqrt{w_{ii}}} \sim t_{n-p}
\]
$1 - \alpha$ Confidence intervals for new observation $Y_h$ at $x_h$ and $E[Y_h]$:
\[
    E[y_h] \approx \hat{y_h} \pm t(n-p, 1 - \frac{\alpha}{2}) \hat{\sigma}
        \sqrt{x_h^T (X^T X)^{-1} x_h}
\]
\[
    y_h \approx \hat{y_h} \pm t(n-p, 1 - \frac{\alpha}{2}) \hat{\sigma}
        \sqrt{1 + x_h^T (X^T X)^{-1} x_h}
\]

General linear tests. Partition $\beta = (\beta_1, \beta_2)$ where $\beta_1$
is an $r$ vector and $\beta_2$ is $p - r$. Null hypothesis $H_0: \beta_2 =
\beta_2^*$ (often 0), and $H_a: \beta_2 \neq \beta_2^*$. Then
$SSE_r = \norm{y - X_2 \beta_2^* - X_1 \tilde{\beta_1}}^2$ is the sum of
squared error for the reduced model and 
$SSE_f = \norm{y - X \hat{\beta}}^2$ is the squared sum of error for the
full model.
Under $H_0$:
\[
    \frac{\frac{SSE_r - SSE_f}{p - r}}
         {\frac{SSE_f}{n - p}}
         \sim F_{p-r, n-p}
\]

Alternate forms of linear test, and testing a linear combination if $R\beta
= r$.
\[
    \frac{(R \hat{\beta} - r)^T (R(X^T X)^{-1} R^T)^{-1} (R \beta - r) / s}
    {\hat{\sigma}}^2 \sim F_{s, n-p}
\]

Simultaneous confidence bands for $\beta$.

\subsection*{Model selection and diagnostics}

$SSTO = \sum_{i=1}^n (y_i - \bar{y}) = \norm{y - \bar{y} 1_n }^2
        = \norm{(I - J)y}^2$

$SSR = \sum_{i=1}^n (\hat{y}_i - \bar{y}) = \norm{\hat{y} - \bar{y} 1_n}^2
    = \norm{(H - P)y}^2$

$SSE = \sum_{i=1}^n (y_i - \hat{y}_i) = \norm{y - \hat{y}}^2
        = \norm{(I - H)y}^2$

If the model contains the intercept in the column space of $X$  then $SSTO = SSR + SSE$.

$R^2 = 1 - \frac{SSE}{SSTO}$

Adjusted $R^2_a = 1 - \frac{SSE / (n-p)}{SSTO / (n-1)}$

$AIC = n \log SSE + 2p$

$BIC = n \log SSE + p \log n$

$Cp = \frac{SSE}{MSE} - (n - 2p)$

Residuals: $\hat{\epsilon}_i = y_i - \hat{y}_i$

Studentized residuals (\texttt{rstandard} in R): 
\[
    \gamma_i =
    \frac{\hat{\epsilon}_i}{ s \{ \hat{\epsilon}_i \} } = 
    \frac{\hat{\epsilon}_i}{\hat{\sigma} \sqrt{1 - h_{ii}}}
\]

Prediction sum of squares (PRESS) is the same as leave one out cross
validation (LOOCV). Prediction error on $i$th observation is called deleted
residuals:
\[
    y_i - \hat{y}_{i (-i)} = \frac{y_i - \hat{y}_i}{1 - H_{ii}}
\]
Works for ridge regression also, letting 
$H = X(X^T X + \lambda I)^{-1} X^T$.

Studentized deleted residuals: 
\[
    t_i = \frac{\hat{\epsilon}_i}{\sqrt{MSE_{(-i)} (1 - h_{ii})}} \sim t_{n - p -1}
\]
Where $MSE_{(-i)} = SSE_{(-i)} / (n - 1 - p)$ and
$SSE_{(-i)} = SSE - \frac{\hat{\epsilon}_i}{1 - h_{ii}}$ can be used to
calculate without refitting model.

\subsection*{ANOVA}

Three principles of experimental design: 1) Replication 2) Randomization 3)
Blocking

One way ANOVA with $n$ total observations, $K$ groups:

{
\centering
\begin{tabular}{lll}
    SS   &  & DF     \\
    SSTR & $\sum_{j=1}^K n_j (\bar{y_{j \cdot}} - \bar{y_{\cdot \cdot}})^2$  & K - 1 \\
    SSE  & $\sum_{i=1}^n (y_{ij} - \bar{y_{j \cdot}})^2$  & n - K \\
    SSTO & $\sum_{i=1}^n (y_{ij} - \bar{y_{\cdot \cdot}})^2$  & n - 1 
\end{tabular}
}

Contrasts are sums of the form $\Phi = \sum_{i=1}^K c_i \mu_i$ with
$\sum_{i=1}^K c_i = 0$.
Tukey's works for all pairwise contrasts.
Scheffe's and extended Tukey works for all contrasts.
Bonferroni's is for a limited number of pre specified contrasts.

\textbf{Ridge Regression} for $\lambda > 0$ solves
\[
    \min_\beta ||Y - X\beta||^2 + \lambda ||\beta||^2
\]

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Applied Lectures - 232B}

4 Jan - Matrix form for linear mixed model, assumptions

\subsection*{Math Lectures - 231B}

4 Jan - Maximum likelihood, determining MLE

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Applied Lectures - 232A}

30 Nov - Outliers in x and y, leverage, DFFITS, cook's distance, influence
plot, add variable plot, robust regression

25 Nov - Model selection criteria, deleted residuals, forward and backward
selection in lab

23 Nov - BIC, AIC derivations, Mallow's cp, stepwise selection algorithms,
outliers and studentized residuals

18 Nov - F test with orthogonalized X, model selection criteria,
bootstrap t method in lab

16 Nov - Multicollinearity, Variance Inflation Factor,
ridge regression, bias variance tradeoff, AIC, BIC, cross validation, proof leave
one out cross validation formula for OLS 

11 Nov - Holiday

9 Nov - Linear model with random $X$, transformations of $y$ and $X$, box
cox procedure, bootstrap with percentile-t and fixed $X$ sampling, weighted
least squares

4 Nov - Interaction plots for two way ANOVA with balanced design, Linear
models with random $X$

2 Nov - Midterm

28 Oct - Kronecker product formulae for two way ANOVA

26 Oct - Kronecker product 1 way ANOVA, decomposition of two way ANOVA,
noncentral $\chi^2$ distributions for ANOVA table SSA, SSB, SSAB

21 Oct - Tukey's method for pairwise contrasts, Bonferroni's method,
definition and properties of Kronecker product

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Math Lectures - 231A}

30 Nov - Bayesian inference, risk, decision rules, conjugate families,
Binomial, normal examples

25 Nov - Exponential families, GLM's, full rank exp families, decision
theory, Bayes risk

23 Nov - Fisher information, Cramer Rao inequality, Exponential families
and properties

18 Nov - Rao-Blackwell theorem, Lehmann-Scheffe theorem, UMVUE examples for
normal, uniform, Poisson, Fisher information

16 Nov - Minimal sufficiency, likelihood ratio, ancillary statistics,
completeness, Basu's theorem, loss functions

11 Nov - Holiday

9 Nov - Distribution of order statistics, factorization theorem, sufficient
statistics for Exponential families and uniform dist

4 Nov - Midterm

2 Nov - Location-scale families, invariance, ancillary and sufficient
statistics, order statistics, multinomial distribution

28 Oct - Transformation of discrete and continuous random variables,
Jacobian, examples with beta distributions, Dirichlet distribution

26 Oct - Jensen's and Holder's inequality, convex functions and sets,
products of normal random variables

21 Oct - Convolution formula, examples with Uniform, Gamma, Poisson,
marginal and conditional distributions for multivariate normal
\newpage

\subsection*{Problem Solving Strategies}

\textbf{Read the whole question}

Read carefully and do the right problem! If there's a hint, it should
probably be used. Early parts of a question can help for later parts, and later
parts occasionaly provide insight for earlier parts.

\textbf{First principles}

When in doubt, work from definitions.

\textbf{Look for Distributions} 

Can the question be solved by knowing the distribution of some
quantity?  Ex: $\sum (x_i - \bar{x})^2$ is $\chi^2_{n-1}$ for
$x_i \sim N(\mu, 1)$.

\textbf{Fast and correct algebra}

Better to write more than to make a simple algebra mistake.
Practice common manipulations so don't have to think about them
when testing.

\begin{table}[]
    \centering
    \caption{Problems in past 231 exams - Came from a brief glance at the
    question statements. TODO- make second updated table after solving
    questions that shows which techniques are used.}
    \label{231problems}
    \begin{tabular}{rl}

        Binomial                & ******* \\
        Poisson                 & ****** \\
        Uniform                 & ****** \\
        Normal                  & ****** \\
        Gamma                   & *** \\
        Exponential             & ** \\
        Negative binomial       & * \\
        Beta                    & * \\
        Geometric               & * \\
        MLE                     & ************* \\
        asymptotic distribution & *********** \\
        Bayes estimator / risk  & ********** \\
        UMVUE / Cramer-Rao      & ********* \\
        minimiax                & ****** \\
        UMP test                & ****** \\
        linear regression       & ***** \\
        likelihood ratio        & **** \\
        Wald's test             & *** \\
        sufficient statistic    & ** \\
        Hierarchical model      & * \\
        method of moments       & * \\
        hypothesis testing      & * \\
        order statistics        & * \\

    \end{tabular}
\end{table}

\end{document}
