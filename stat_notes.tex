% A simple template for LaTeX documents
% 
% To produce pdf run:
%   $ pdflatex paper.tex 
%

\documentclass[10pt, twocolumn]{article}
%\documentclass[12pt]{article}

% Begin paragraphs with new line
\usepackage{parskip}  

% Change margin size
\usepackage[margin=0.5in]{geometry}   

% Graphics Example:  (PDF's make for good plots)
\usepackage{graphicx}               
% \centerline{\includegraphics{figure.pdf}}

% Allows hyperlinks
\usepackage{hyperref}

% Blocks of code
\usepackage{listings}
\lstset{basicstyle=\ttfamily, title=\lstname}
% Insert code like this. replace `plot.R` with file name.
% \lstinputlisting{plot.R}

% Monospaced fonts
%\usepackage{inconsolata}
% GNU \texttt{make} is a nice tool.

% Supports proof environment
\usepackage{amsthm}

% Allows writing \implies and align*
\usepackage{amsmath}

% Allows mathbb{R}
\usepackage{amsfonts}

% Numbers in scientific notation
\usepackage{siunitx}

% Use tables generated by pandas
\usepackage{booktabs}

% norm and infinity norm
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\inorm}[1]{\left\lVert#1\right\rVert_\infty}

% Statistics essentials
\newcommand{\iid}{\text{ iid }}
\newcommand{\Expect}{\operatorname{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\textbf{Binomial}
$X \sim B(n, p)$
\[
    p(k) = \binom{n}{k} p^k (1-p)^{n-k}
    \qquad k = 0, 1, \dots, n
\]
$\Expect X = np, \quad \Var X = np(1-p)$

mgf: $M_X (t) = (pe^t + 1 - p)^n$

Beta is conjugate prior, Fisher info $I(p) = \frac{1}{p(1 - p)}$

\textbf{Poisson}
$X \sim P(\lambda)$
\[
    p(k) = \frac{e^{-\lambda} \lambda^k}{k!}
    \qquad k = 0, 1, \dots
\]
$\Expect X = \lambda, \quad \Var X = \lambda$

mgf: $M_X (t) = e^{\lambda (e^t -1)}$ Use recursive relation to compute
$\Expect(X_i)$.

Gamma is conjugate prior, Fisher info $I(\lambda) = \frac{1}{\lambda}$

\textbf{Normal}
$X \sim N(\mu, \Sigma)$, $\Sigma$ positive definite
\[
    f(x) = \frac{\exp\{ - \frac{1}{2}(x - \mu)^T \Sigma^{-1} (x - \mu) \}}
        {(2\pi)^{\frac{k}{2}} \sqrt{\det(\Sigma)}}
        = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x - \mu)^2}{2
        \sigma^2}}
\]
mgf: $M_X (t) = \exp (\mu' t + \frac{1}{2} t' \Sigma t)$

Normal is conj. prior, Fisher info $I(\mu, \sigma^2) = 
[\begin{smallmatrix}
        1 / \sigma^2 & 0 \\
        0 & 1 / 2\sigma^4 \\
\end{smallmatrix}]$

\textbf{Beta}
$ X \sim \text{Beta}(\alpha, \beta)$
\[
    f(x) = \frac{x^{\alpha-1}(1 - x)^{\beta-1}}{B(\alpha, \beta)} 
    \qquad 0 \leq x \leq 1
\]
$\Expect X = \frac{\alpha}{\alpha + \beta},
\quad \Var X = \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}$

using the beta function:
\[
    B(\alpha, \beta) =
    \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha+ \beta)} =
    \int_0^1 t^{\alpha -1} (1-t)^{\beta - 1}dt
\]

\textbf{Gamma}
$X \sim \text{Gamma}(\alpha, \beta)$
\[
    f(x) = \frac{\beta^\alpha x^{\alpha-1} e^{-\beta x}}{\Gamma(\alpha)}
    \qquad x > 0
\]
$\Expect X = \frac{\alpha}{\beta},
\quad \Var X = \frac{\alpha}{\beta^2}$

mgf: $M_X (t) = (1 - \frac{t}{\beta})^{-\alpha}, t < \beta$

$X \sim \text{Gamma}(\alpha, \beta) \iff \beta X \sim \text{Gamma}(\alpha, 1)$

$X_i \iid \text{Gamma}(\alpha_i, \beta)$, then
\[
    \sum X_i \sim \text{Gamma}(\sum \alpha_i, \beta)
\]
Gamma function: $\Gamma(\alpha) = \int_0^\infty t^{\alpha-1} e^{-t} dt$.

$\Gamma(\frac{1}{2}) = \sqrt{\pi}$.

$\Gamma(\alpha + 1) = \alpha \Gamma(\alpha)$

$\Gamma(k) = (k-1)!$ for $k$ positive integer.

\textbf{Exponential}
Special case: $X \sim \text{Exp}(\lambda) \equiv \text{Gamma}(1, \lambda)$

$
f(x) = \lambda e^{-\lambda x},
\quad x > 0
\qquad \Expect X = \frac{1}{\lambda},
\quad \Var X = \frac{1}{\lambda ^2}
$

CDF $F(x) = 1 - e^{-\lambda x}$

\textbf{Chi square}
Special case: $X \sim \chi^2_n \equiv \text{Gamma}(\frac{n}{2}, \frac{1}{2})$

$
f(x) \propto x^{\frac{n}{2} - 1} e^{\frac{-x}{2}},
\quad x > 0
\qquad \Expect X = n,
\quad \Var X = 2n
$

Let $Z_i$ be iid $N(0, 1)$.

$\sum_{i=1}^n Z_i^2 \sim \chi^2_n$

Noncentral $\chi^2$. Let $Y \sim N(\mu, I)$ be an $n$ vector. Then 
\[
    \norm{Y}^2 \sim \chi^2_n(\norm{\mu}^2)
\]

\textbf{F}
\[
    F(m, n) \equiv \frac{\frac{\chi^2_m}{m}}
        {\frac{\chi^2_n}{n}}
\]
Where numerator and denominator are independent $\chi^2$.

\textbf{T}
\[
    t(n) = \frac{N(0, 1)}
    {\sqrt{\frac{\chi^2_n}{n}}}
\]
Where numerator and denominator are independent.

\vspace{0.2in}
\hrule

\textbf{Transformations} If $g$ 1:1 with continuous derivatives and nonzero
Jacobian, and $Y = g(X)$, then the density
\[
    f_Y(y) = f_X(g^{-1}(y)) |J_{g^{-1}}(y)|
\]
For affine transformation $Y = AX + c$ then 
\[
    f_Y(y) = f_X(A^{-1}(y - c)) |\det A|^{-1}
\]

Moment generating functions determine distribution
\[
    M_X(t) \equiv \Expect (e^{tX}),
    \quad M_X'(0) = \Expect(X)
\]
$X_i$ independently distributed $\iff$
\[
    M_{\sum X_i} (t) = \prod M_{X_i} (t)
\]
\textbf{Characteristic function}
\[
    \phi(t) = \Expect (e^{i t^T X)}
    = \Expect (\cos (t^T X)) + i \Expect(\sin(t^T X))
\]
Order statistics for sorted sample $X_{(1)}, \dots, X_{(n)}$ has pdf:
\[
    n! \prod_{i=1}^n f(X_{(i)}) \quad I(X_{(1)} < \dots < X_{(n)})
\]

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{TODO - add measure theory}

\textbf{Jensen's Inequality} if $S \subset R^k$ convex and closed, $g$ convex on $S$, $P[X
\in S] = 1$, and $\Expect X$ is finite, then $\Expect X \in S$, $\Expect
g(X)$ exists, and
\[
    \Expect g(X) \geq g(\Expect X)
\]
\textbf{Holder's Inequality} if $r, s > 1$ and $\frac{1}{r} + \frac{1}{s} =
1$ then
\[
    \Expect |XY| \leq (\Expect |X|^r)^{\frac{1}{r}}(\Expect |X|^s)^{\frac{1}{s}}
\]

$T(X)$ Sufficient means the distribution of $X | T(X)$ does not depend on
$\theta$.

Factorization theorem: $T(x)$ is sufficient $\iff$
\[
    f_{\theta}(x) = h(x) g(\theta, T(x))
\]
$L_x (\theta) = p_\theta (x) = p(x, \theta)$ likelihood is function of
$\theta$, density is function of $x$.

The likelihood ratio
\[
    \lambda_x (\theta) = \frac{L_x (\theta)}{L_x (\theta_0)}
\]
is minimal sufficient. To show $T(x)$ is minimal sufficient show that it is
sufficient and a function of the likelihood $\lambda_x (\theta)$.

\textbf{Fisher information}
\[
    I(\theta) = \Expect_\theta \left[ \frac{\partial}{\partial \theta} 
    \log L_X (\theta) \right]^2
    = \Expect_\theta \left[ - \frac{\partial^2}{\partial \theta^2}
    \log L_X (\theta) \right]
\]
$\text{bias } \hat{v} \equiv \Expect (\hat{v}) - v$
\[
    MSE(\hat{v}) \equiv \Expect (\hat{v} - v)^2 
    = \Var (\hat{v}) + (\text{bias } \hat{v})^2
\]
\textbf{Rao-Blackwell} Let $S(X)$ be an unbiased point estimator for
$g(\theta)$. Conditioning on a sufficient statistic $T(X)$ reduces
variance.
\[
    \Var_{\theta} (S(X)) \geq \Var_{\theta} (\Expect (S(X) | T(X)))
\]
Also holds for more general convex loss function $L$:
\[
    R(\theta, S) \equiv \Expect_{\theta} L(\theta, S(X)) \geq
    \Expect_{\theta} L(\theta, \Expect( S(X) | T(X)))
\]
\textbf{Completeness} $T(X)$ is complete if $\Expect g(T(X)) = 0$ 
implies $g = 0$ almost surely for all $\theta$.

\textbf{Cramer Rao Inequality} Let $g: \Theta \rightarrow R$. Suppose there
exists an unbiased estimator $U(X)$, $\Expect U(X) = g(\theta)$. Then
\[
    \Var_\theta U(X) \geq 
    \left( \frac{\partial g(\theta)}{\partial \theta} \right)^T
    I(\theta)^{-1}
    \left( \frac{\partial g(\theta)}{\partial \theta} \right)
\]
Basu's Theorem - If $T(X)$ complete sufficient statistic and $A(X)$ is 
ancillary then $A(X)$ and $T(X)$ are independent.

\textbf{Lehmann - Scheffe} Suppose $T(X)$ is complete sufficient. Then
there exists unique unbiased estimator $\Expect h(T(x))$ of $g(\theta) \in
R$ with smallest variance (MVUE). 

\textbf{Exponential Families} $T(x)$ is natural sufficient statistic and is
complete sufficient if the $k$ parameter exponential family is full rank.
\[
    p(x, \theta) = h(x) \exp \{ \eta(\theta)^T T(x) - B(\theta) \}
\]
Canonical form model indexed by $\eta$.
\[
    q(x, \eta) = h(x) \exp \{ \eta^T T(x) - A(\eta) \}
\]
\[
    \dot{A} (\eta) = \Expect_\eta (T(X)) \quad
    \ddot{A} (\eta) = I(\eta) = \Var_\eta (T(X))
\]
Then moment generating function for $T(X)$ is
\[
    M_{T(X)}(t) = \exp \{A(t + \eta) - A(\eta) \}
\]
Equivalent statements useful for GLM's such as $Y \sim N(X \beta,
\sigma_0^2 I)$, where $Z$ is $n \times p$:

1. $I(\beta) = \frac{1}{\sigma_0^2} X^T X$ positive definite
2. rank$(X) = p$
3. model is identifiable. More generally another equivalent statement is
$\Var (T(X)) = \ddot{A} (\eta)$ is positive definite.

\subsection*{Decision Theory}

\textbf{Decision rule} $\delta: \mathcal{X} \rightarrow \mathcal{A}$, where 
$\delta \in \mathcal{D}$, the space of possible decision rules and
$\mathcal{A}$ is the action space.

\textbf{Loss function} $l: \Theta \times \mathcal{A} \rightarrow \mathbb{R}^+$
Posterior mean minimizes square error loss; median minimizes absolute loss.

\textbf{Risk function} $R: \Theta \times \mathcal{D} \rightarrow
\mathbb{R}^+$ expected loss for a particular value of $\theta$
\[
    R(\theta, \delta) = \Expect_\theta l(\theta, \delta(X)) = \int
    l(\theta, \delta(x)) \cdot p_\theta (x) dx
\]
Bayes setup:
\[
    \pi(\theta | x) = \frac{p_\theta (x) \pi(\theta)}{m(x)}
\]
\textbf{Bayes decision rule} If there exists $\delta_\pi \in \mathcal{D}$
w.r.t prior $\pi$ such that
\[
    r(\pi, \delta_\pi) = \inf_{\delta \in \mathcal{D}} r(\pi, \delta)
\]
To find Bayes rule minimize the posterior risk:
\[
    \delta_\pi (x) = \min_{a \in \mathcal{A}} r_\pi (a | x)
\]

\textbf{Bayes risk} $r_\pi : \mathcal{D} \rightarrow
\mathbb{R}^+$ expected loss for fixed prior $\pi$
\[
    r_\pi (\delta) = \Expect_\pi R(\theta, \delta) 
    = \int_\Theta R(\theta, \delta) \pi (d \theta)
    = \int_\mathcal{X} r_\pi(\delta(x) | x) m(x) dx
\]
To find Bayes risk: 1) find the Bayes rule 2) compute the risk function 3)
take the expectation of the risk wrt prior $\pi$.


\textbf{Minimax} decision rule $\delta^*$ minimizes the worst case
scenario, satisfies
\[
    \sup_{\theta} R(\theta, \delta^*) = \inf_{\delta} \sup_{\theta}
    R(\theta, \delta)
\]
To show $\delta^*$ is minimax, first check for constant risk $R(\theta,
\delta^* = c$ for all $\theta$, then find a
prior $\pi$ such that $\delta^*$ is the Bayes rule. This $\pi$ is least
favorable. More generally can find
a sequence of priors $(\pi_k)$ such that the Bayes risk $r_{\pi_k}
(\delta_{\pi_k}) \rightarrow c$.

\newpage
\subsection*{Asymptotics}

\textbf{Almost Sure convergence}
\[
    X_n \xrightarrow{a.s.} X \quad \text{means} \quad P(X_n \rightarrow X)
    = 1
\]
Theorem: $\iff P(\sup_{m \geq n} |X_m - X| > \epsilon) \rightarrow 0
\quad \forall \epsilon > 0$.

\textbf{Convergence in Probability}
\[
    X_n \xrightarrow{p} X
\]
means that $P( |X_n - X| > \epsilon ) \rightarrow 0$ for all $\epsilon > 0$.

\textbf{Generalized Chebychev Inequality}
Let $X$ be a r.v. and $g$ be a
nonnegative function increasing on the range of $X$. Then
\[
    P(X \geq a) \leq \frac{\Expect g(X)}{g(a)}
\]

\textbf{Borel Cantelli Lemma}

\textbf{Hoeffding Inequality}
Let $X_1, \dots, X_n$ be independent (not necessarily iid) with $a_i \leq
X_i \leq b_i$ and $\Expect X_i = 0$. Then 
\[
    P \left( \sum_{i=1}^n X_i \geq \eta \right) \leq 
    \exp \left\{ -\frac{2\eta^2}{\sum_{i=1}^n (b_i - a_i)^2} \right\}
\]

\newpage
\subsection*{Multivariate Normal}

log likelihood for $k$ vector $x \sim N(\mu, \Sigma)$
\[
    l_x = -\frac{k}{2} \log 2 \pi - \frac{1}{2}
    \{ \log \det \Sigma + (x - \mu)^T \Sigma^{-1} (x - \mu) \}
\]
Stein's formula: $X \sim N(\mu, \sigma)$
\[
    \Expect (g(X) (X - \mu)) = \sigma^2 \Expect(g'(X))
\]
assuming these expectations are finite.

$X \sim N(\mu, \Sigma)$, $A$ an $m \times n$ matrix,
then 
\[
    AX \sim N(A \mu, A \Sigma A^t)
\]
For $\Sigma$ full rank it's possible to transform between $Z \sim
N(0, I)$ and $X$:
\[
    X = \Sigma^{1/2} Z + \mu \qquad Z = \Sigma^{-1/2} (X - \mu)
\]
In block matrix form:
\[
    X =
    \begin{bmatrix}
        X_1 \\
        X_2 \\
    \end{bmatrix}
    \sim N \left(
    \begin{bmatrix}
        \mu_1 \\
        \mu_2 \\
    \end{bmatrix}
    ,
    \begin{bmatrix}
        \Sigma_{11} & \Sigma_{12} \\
        \Sigma_{21} & \Sigma_{22} \\
    \end{bmatrix}
\right)
\]
Assuming $\Sigma_{11}$ is positive definite then the conditional
distribution
\[
    X_2 | X_1 \sim N(\mu_2 + \Sigma_{21} \Sigma_{11}^{-1} (X_1 - \mu_1),
    \Sigma_{22} - \Sigma_{21} \Sigma_{11}^{-1} \Sigma_{12})
\]

\subsection*{Conditional Distributions}

Conditional pdf:
\[
    f_{X|Y}(x | y) \equiv \frac{f_{X, Y}(x, y)}{f_Y(y)}
\]
Iterated expectation:
\[
    E(Y) = E(E(Y | X))
\]
Conditional variance formula:
\[
    \Var(Y) = \Var(E(Y | X)) + E(\Var(Y | X))
\]

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{General Techniques}

Singular Value Decompostion (SVD) Any matrix $X$ can be written
\[
    X = UDV^T
\]
with $U, V$ orthogonal, and $D$ diagonal.

Moore Penrose Psuedoinverse $A^+$ exists uniquely for every matrix $A$.

Projection matrix $P$ are symmetric and idempotent. They have eigenvalues
either 0 or 1.
\[
    P = P^T \qquad P^2 = P
\]

Covariance of linear transformations
\[
    Cov(Ay, Bx) = A Cov(y, x) B^T
\]
Invert $2 \times 2$ matrix:
$
    A = 
    [\begin{smallmatrix}
        a & b \\
        c & d \\
    \end{smallmatrix}]
$
\[
    A^{-1} = 
    \frac{1}{\det (A)}
    \begin{bmatrix}
        d & -b \\
        -c & a \\
    \end{bmatrix}
\]

Sum identities:
\[
    \sum_{k=0}^{\infty} p^k = \frac{p}{1 - p} \qquad 
    \sum_{k=0}^{\infty} k p^k = \frac{p}{(1 - p)^2} \qquad |p| < 1
\]

Integration by parts:
\[
    \int uv' = uv - \int u'v
\]

Matrix / Vector differentiation

$\frac{\partial A^T \beta}{\partial \beta} = A$, 
$\frac{\partial \beta^T A \beta}{\partial \beta} = (A + A^t) \beta =
2A\beta$ for $A$ symmetric.

$\frac{\partial}{\partial \theta_i} \log (|A|) =
tr( A^{-1} \frac{\partial A}{\partial \theta_i}$)

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Linear Models}

Least Squares Principle
\[
    \text{arg min}_\beta \norm{Y - X\beta}^2
\]

Normal Model 
\[
    Y = X\beta + \epsilon, \qquad \epsilon \sim N(0, \sigma^2 I)
\]

Normal Equations - Any $b$ satistfying this solves the least squares
\[
    X^T X b = X^T y
\]

Gauss Markov Theorem - $\hat{\beta}$ is Best Linear Unbiased
Estimator (BLUE) of $\beta$.
\[
    \hat{\beta} = (X^T X)^{-1} X^T y \sim N(\beta, \sigma^2 (X^T X)^{-1})
\]

Estimating the variance: $\frac{\norm{y - X \hat{\beta}}^2}{\sigma^2} \sim
\chi^2_{n-p}$.
\[
    \hat{\sigma}^2 = \frac{\norm{y - X \hat{\beta}}^2}{n - p}
\]

Use t test for hypothesis testing and confidence intervals for the value of
a particular $\beta_j$ coefficient. 
Let $w_{ii}$ be the $i$th diagonal entry of $(X^T X)^{-1}$.
\[
    \frac{\beta_j - \beta_j^*}{\hat{\sigma} \sqrt{w_{ii}}} \sim t_{n-p}
\]
$1 - \alpha$ Confidence intervals for new observation $Y_h$ at $x_h$ and $E[Y_h]$:
\[
    E[y_h] \approx \hat{y_h} \pm t(n-p, 1 - \frac{\alpha}{2}) \hat{\sigma}
        \sqrt{x_h^T (X^T X)^{-1} x_h}
\]
\[
    y_h \approx \hat{y_h} \pm t(n-p, 1 - \frac{\alpha}{2}) \hat{\sigma}
        \sqrt{1 + x_h^T (X^T X)^{-1} x_h}
\]
Simultaneous (Working-Hotelling) confidence interval for $\Expect (y_h)$:
\[
    \hat{y}_h \pm \sqrt{p F_{p, n - p, 1 - \alpha}} se\{ \hat{y}_h \}
\]
\[
    \frac{(\hat{\beta} - \beta)^T X^T X (\hat{\beta} - \beta) /
    p}{\hat{\sigma}^2}
    \sim F_{p, n - p}
\]

General linear tests. Partition $\beta = (\beta_1, \beta_2)$ where $\beta_1$
is an $r$ vector and $\beta_2$ is $p - r$. Null hypothesis $H_0: \beta_2 =
\beta_2^*$ (often 0), and $H_a: \beta_2 \neq \beta_2^*$. Then
$SSE_r = \norm{y - X_2 \beta_2^* - X_1 \tilde{\beta_1}}^2$ is the sum of
squared error for the reduced model and 
$SSE_f = \norm{y - X \hat{\beta}}^2$ is the squared sum of error for the
full model.
Under $H_0$:
\[
    \frac{\frac{SSE_r - SSE_f}{p - r}}
         {\frac{SSE_f}{n - p}}
         \sim F_{p-r, n-p}
\]

Alternate forms of linear test, and testing a linear combination if $R\beta
= r$, for $R$ full rank $s \times p$ matrix.
\[
    \frac{(R \hat{\beta} - r)^T (R(X^T X)^{-1} R^T)^{-1} (R \beta - r) / s}
    {\hat{\sigma}^2} \sim F_{s, n-p}
\]


\subsection*{Model selection and diagnostics}

$SSTO = \sum_{i=1}^n (y_i - \bar{y}) = \norm{y - \bar{y} 1_n }^2
        = \norm{(I - J)y}^2$

$SSR = \sum_{i=1}^n (\hat{y}_i - \bar{y}) = \norm{\hat{y} - \bar{y} 1_n}^2
    = \norm{(H - P)y}^2$

$SSE = \sum_{i=1}^n (y_i - \hat{y}_i) = \norm{y - \hat{y}}^2
        = \norm{(I - H)y}^2$

If the model contains the intercept in the column space of $X$  then $SSTO = SSR + SSE$.

$R^2 = 1 - \frac{SSE}{SSTO}$

Adjusted $R^2_a = 1 - \frac{SSE / (n-p)}{SSTO / (n-1)}$

$AIC = n \log SSE + 2p$

$BIC = n \log SSE + p \log n$

$Cp = \frac{SSE}{MSE} - (n - 2p)$

Residuals: $\hat{\epsilon}_i = y_i - \hat{y}_i$

Studentized residuals (\texttt{rstandard} in R): 
\[
    \gamma_i =
    \frac{\hat{\epsilon}_i}{ s \{ \hat{\epsilon}_i \} } = 
    \frac{\hat{\epsilon}_i}{\hat{\sigma} \sqrt{1 - h_{ii}}}
\]

Prediction sum of squares (PRESS) is the same as leave one out cross
validation (LOOCV). Prediction error on $i$th observation is called deleted
residuals:
\[
    y_i - \hat{y}_{i (-i)} = \frac{y_i - \hat{y}_i}{1 - H_{ii}}
\]
Works for ridge regression also, letting 
$H = X(X^T X + \lambda I)^{-1} X^T$.

Studentized deleted residuals: 
\[
    t_i = \frac{\hat{\epsilon}_i}{\sqrt{MSE_{(-i)} (1 - h_{ii})}} \sim t_{n - p -1}
\]
Where $MSE_{(-i)} = SSE_{(-i)} / (n - 1 - p)$ and
$SSE_{(-i)} = SSE - \frac{\hat{\epsilon}_i}{1 - h_{ii}}$ can be used to
calculate without refitting model.

\subsection*{ANOVA}

Three principles of experimental design: 1) Replication 2) Randomization 3)
Blocking

One way ANOVA with $n$ total observations, $K$ groups:

{
\centering
\begin{tabular}{lll}
    SS   &  & DF     \\
    SSTR & $\sum_{j=1}^K n_j (\bar{y_{j \cdot}} - \bar{y_{\cdot \cdot}})^2$  & K - 1 \\
    SSE  & $\sum_{i=1}^n (y_{ij} - \bar{y_{j \cdot}})^2$  & n - K \\
    SSTO & $\sum_{i=1}^n (y_{ij} - \bar{y_{\cdot \cdot}})^2$  & n - 1 
\end{tabular}
}

Contrasts are sums of the form $\Phi = \sum_{i=1}^K c_i \mu_i$ with
$\sum_{i=1}^K c_i = 0$.
Tukey's works for all pairwise contrasts.
Scheffe's and extended Tukey works for all contrasts.
Bonferroni's is for a limited number of pre specified contrasts.

\textbf{Ridge Regression} for $\lambda > 0$ solves
\[
    \min_\beta ||Y - X\beta||^2 + \lambda ||\beta||^2
\]

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Linear mixed models}
\[
    y = X \beta + Z \alpha + \epsilon
\]
Standard assumptions:

$\Expect ( \alpha) = 0$, $\Expect ( \epsilon) = 0$,
$\Cov(\epsilon, \alpha) = 0$

$\alpha \sim N(0, G), \epsilon \sim N(0, R)$ are jointly normal.

Marginal model $y \sim N(X \beta, V)$ where $V = R + ZGZ'$.

The idea of reduced maximum likelihood (REML) is to first estimate the random components of the model.
This is done through a transformation of the data to create a new model
containing only the random components, and no fixed components.

Let $\xi = b' \beta + a' \alpha$ be a mixed effect. These are what we're
interested in estimating. We call them predictions rather than estimations
because we're predicting a random component.

BLUE - Best linear unbiased estimator, 
$\tilde{\beta} = (X' V^{-1} X)^{-1} X' V^{-1} y$. This is the MLE of
$\beta$.

BP - Best predictor, $\Expect (\xi | y) = b' \beta + a' G Z' V^{-1} (y - X
\beta)$ a theoretical ideal that's
usually difficult or impossible to derive.

BLUP - Best linear predictor, which plugs in $\tilde{\beta}$ into BP.

EBLUP - Empirical best linear predictor, plugs in $\hat{\theta}$. This is
typically the one we compute and use.


\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Applied Lectures - 232B}

4 Jan - Matrix form for linear mixed model, assumptions

6 Jan - Examples mixed models, time series, non gaussian, marginal, max
likelihood estimation, REML

11 Jan - Lambs example, parametric bootstrap

13 Jan - Asymptotic covariance matrix of fixed effects $\hat{\beta}$,
computing examples

18 Jan - Holiday

20 Jan - Mixed model prediction, BP, BLUE, BLUP, example algebra in
discussion

25 Jan - Empirical BLUP, Fay Herriot model, jackknife

27 Jan - EBLUP for random effects, intro to GLM's

1 Feb - GLMM with several examples, Monte carlo and the E-M algorithm,
importance sampling

3 Feb - Rejection sampling, Markov Chain Monte Carlo (MCMC), Markov chain
convergence theorem, Gibb's Sampler

8 Feb - Gibbs Sampler, transition kernel, MCMC, threshold model, data
cloning

10 Feb - Midterm

\subsection*{Math Lectures - 231B}

4 Jan - Maximum likelihood, determining MLE

6 Jan - Normal examples MLE, constraints, MLE of pdf $f$ decreasing on $[0,
    \infty)$.

11 Jan - MLE maximizing $\Expect l_X (\theta_0)$, KL divergence and
properties, M estimation

13 Jan - least squares and mean absolute deviation (MAD) as examples of m
estimators, existence and uniqueness theorems for optimization, review of
exponential families

18 Jan - Holiday

20 Jan - Existence and uniqueness of MLE for canonical and curved
exponential families, review convexity

25 Jan - Long proof of existence and uniqueness of MLE

27 Jan - Z estimation aka estimating equation estimation, method of
moments, gamma example, plug in principle

1 Feb - Asymptotics, modes of convergence and associated theorems,
Borel-Cantelli lemma, generalized Chebyshev

3 Feb - Hoeffding's inequality and proof, strong, weak, and uniform
consistency

8 Feb - Consistency theorem for MLE's in canonical exp families,
consistency of M estimator


\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Applied Lectures - 232A}

30 Nov - Outliers in x and y, leverage, DFFITS, cook's distance, influence
plot, add variable plot, robust regression

25 Nov - Model selection criteria, deleted residuals, forward and backward
selection in lab

23 Nov - BIC, AIC derivations, Mallow's cp, stepwise selection algorithms,
outliers and studentized residuals

18 Nov - F test with orthogonalized X, model selection criteria,
bootstrap t method in lab

16 Nov - Multicollinearity, Variance Inflation Factor,
ridge regression, bias variance tradeoff, AIC, BIC, cross validation, proof leave
one out cross validation formula for OLS 

11 Nov - Holiday

9 Nov - Linear model with random $X$, transformations of $y$ and $X$, box
cox procedure, bootstrap with percentile-t and fixed $X$ sampling, weighted
least squares

4 Nov - Interaction plots for two way ANOVA with balanced design, Linear
models with random $X$

2 Nov - Midterm

28 Oct - Kronecker product formulae for two way ANOVA

26 Oct - Kronecker product 1 way ANOVA, decomposition of two way ANOVA,
noncentral $\chi^2$ distributions for ANOVA table SSA, SSB, SSAB

21 Oct - Tukey's method for pairwise contrasts, Bonferroni's method,
definition and properties of Kronecker product

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Math Lectures - 231A}

30 Nov - Bayesian inference, risk, decision rules, conjugate families,
Binomial, normal examples

25 Nov - Exponential families, GLM's, full rank exp families, decision
theory, Bayes risk

23 Nov - Fisher information, Cramer Rao inequality, Exponential families
and properties

18 Nov - Rao-Blackwell theorem, Lehmann-Scheffe theorem, UMVUE examples for
normal, uniform, Poisson, Fisher information

16 Nov - Minimal sufficiency, likelihood ratio, ancillary statistics,
completeness, Basu's theorem, loss functions

11 Nov - Holiday

9 Nov - Distribution of order statistics, factorization theorem, sufficient
statistics for Exponential families and uniform dist

4 Nov - Midterm

2 Nov - Location-scale families, invariance, ancillary and sufficient
statistics, order statistics, multinomial distribution

28 Oct - Transformation of discrete and continuous random variables,
Jacobian, examples with beta distributions, Dirichlet distribution

26 Oct - Jensen's and Holder's inequality, convex functions and sets,
products of normal random variables

21 Oct - Convolution formula, examples with Uniform, Gamma, Poisson,
marginal and conditional distributions for multivariate normal
\newpage

\subsection*{Problem Solving Strategies}

\textbf{Read the whole question}

Read carefully and do the right problem! If there's a hint, it should
probably be used. Early parts of a question can help for later parts, and later
parts occasionaly provide insight for earlier parts.

\textbf{First principles}

When in doubt, work from definitions.

\textbf{Look for Distributions} 

Can the question be solved by knowing the distribution of some
quantity?  Ex: $\sum (x_i - \bar{x})^2$ is $\chi^2_{n-1}$ for
$x_i \sim N(\mu, 1)$.

\textbf{Fast and correct algebra}

Better to write more than to make a simple algebra mistake.
Practice common manipulations so don't have to think about them
when testing.

\begin{table}[]
    \centering
    \caption{Problems in past 231 exams - Came from a brief glance at the
    question statements. TODO- make second updated table after solving
    questions that shows which techniques are used.}
    \label{231problems}
    \begin{tabular}{rl}

        Binomial                & ******* \\
        Poisson                 & ****** \\
        Uniform                 & ****** \\
        Normal                  & ****** \\
        Gamma                   & *** \\
        Exponential             & ** \\
        Negative binomial       & * \\
        Beta                    & * \\
        Geometric               & * \\
        MLE                     & ************* \\
        asymptotic distribution & *********** \\
        Bayes estimator / risk  & ********** \\
        UMVUE / Cramer-Rao      & ********* \\
        minimiax                & ****** \\
        UMP test                & ****** \\
        linear regression       & ***** \\
        likelihood ratio        & **** \\
        Wald's test             & *** \\
        sufficient statistic    & ** \\
        Hierarchical model      & * \\
        method of moments       & * \\
        hypothesis testing      & * \\
        order statistics        & * \\

    \end{tabular}
\end{table}

\end{document}
